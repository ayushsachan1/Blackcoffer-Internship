{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Extraction and Text Analysis\n",
    "\n",
    "## Objective \n",
    "\n",
    "Objective of this assignment is to extract some sections (which are mentioned below) from SEC / EDGAR financial reports and perform text analysis to compute variables.\n",
    "\n",
    "## Note\n",
    "\n",
    "Maintain the following directory structure before running this notebook:\n",
    "```bash\n",
    ".\n",
    "├── LoughranMcDonald_MasterDictionary_2018.csv\n",
    "├── Output\\ Data\\ Structure.xlsx\n",
    "├── cik_list.xlsx\n",
    "├── constraining_dictionary.xlsx\n",
    "├── reports\n",
    "│   ├── mda\n",
    "│   ├── qqdmr\n",
    "│   └── rf\n",
    "├── reports_cleaned\n",
    "│   ├── mda\n",
    "│   ├── qqdmr\n",
    "│   └── rf\n",
    "├── stopwords\n",
    "│   ├── StopWords_Auditor.txt\n",
    "│   ├── StopWords_Currencies.txt\n",
    "│   ├── StopWords_DatesandNumbers.txt\n",
    "│   ├── StopWords_Generic.txt\n",
    "│   ├── StopWords_GenericLong.txt\n",
    "│   ├── StopWords_Geographic.txt\n",
    "│   └── StopWords_Names.txt\n",
    "│   \n",
    "└── uncertainty_dictionary.xlsx\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# importing libraries\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import requests\n",
    "from collections import Counter\n",
    "from nltk.tokenize import word_tokenize, sent_tokenize\n",
    "from nltk.corpus import stopwords as nltk_stopwords\n",
    "import pyphen\n",
    "\n",
    "import re\n",
    "from itertools import chain \n",
    "import os\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reading in filenames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>CIK</th>\n",
       "      <th>CONAME</th>\n",
       "      <th>FYRMO</th>\n",
       "      <th>FDATE</th>\n",
       "      <th>FORM</th>\n",
       "      <th>SECFNAME</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>3662</td>\n",
       "      <td>SUNBEAM CORP/FL/</td>\n",
       "      <td>199803</td>\n",
       "      <td>1998-03-06</td>\n",
       "      <td>10-K405</td>\n",
       "      <td>edgar/data/3662/0000950170-98-000413.txt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>3662</td>\n",
       "      <td>SUNBEAM CORP/FL/</td>\n",
       "      <td>199805</td>\n",
       "      <td>1998-05-15</td>\n",
       "      <td>10-Q</td>\n",
       "      <td>edgar/data/3662/0000950170-98-001001.txt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3662</td>\n",
       "      <td>SUNBEAM CORP/FL/</td>\n",
       "      <td>199808</td>\n",
       "      <td>1998-08-13</td>\n",
       "      <td>NT 10-Q</td>\n",
       "      <td>edgar/data/3662/0000950172-98-000783.txt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3662</td>\n",
       "      <td>SUNBEAM CORP/FL/</td>\n",
       "      <td>199811</td>\n",
       "      <td>1998-11-12</td>\n",
       "      <td>10-K/A</td>\n",
       "      <td>edgar/data/3662/0000950170-98-002145.txt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>3662</td>\n",
       "      <td>SUNBEAM CORP/FL/</td>\n",
       "      <td>199811</td>\n",
       "      <td>1998-11-16</td>\n",
       "      <td>NT 10-Q</td>\n",
       "      <td>edgar/data/3662/0000950172-98-001203.txt</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    CIK            CONAME   FYRMO      FDATE     FORM  \\\n",
       "0  3662  SUNBEAM CORP/FL/  199803 1998-03-06  10-K405   \n",
       "1  3662  SUNBEAM CORP/FL/  199805 1998-05-15     10-Q   \n",
       "2  3662  SUNBEAM CORP/FL/  199808 1998-08-13  NT 10-Q   \n",
       "3  3662  SUNBEAM CORP/FL/  199811 1998-11-12   10-K/A   \n",
       "4  3662  SUNBEAM CORP/FL/  199811 1998-11-16  NT 10-Q   \n",
       "\n",
       "                                   SECFNAME  \n",
       "0  edgar/data/3662/0000950170-98-000413.txt  \n",
       "1  edgar/data/3662/0000950170-98-001001.txt  \n",
       "2  edgar/data/3662/0000950172-98-000783.txt  \n",
       "3  edgar/data/3662/0000950170-98-002145.txt  \n",
       "4  edgar/data/3662/0000950172-98-001203.txt  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cik_list = pd.read_excel('data/cik_list.xlsx')\n",
    "cik_list.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 152 entries, 0 to 151\n",
      "Data columns (total 6 columns):\n",
      " #   Column    Non-Null Count  Dtype         \n",
      "---  ------    --------------  -----         \n",
      " 0   CIK       152 non-null    int64         \n",
      " 1   CONAME    152 non-null    object        \n",
      " 2   FYRMO     152 non-null    int64         \n",
      " 3   FDATE     152 non-null    datetime64[ns]\n",
      " 4   FORM      152 non-null    object        \n",
      " 5   SECFNAME  152 non-null    object        \n",
      "dtypes: datetime64[ns](1), int64(2), object(3)\n",
      "memory usage: 7.2+ KB\n"
     ]
    }
   ],
   "source": [
    "cik_list.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Downloading Reports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_url = \"https://www.sec.gov/Archives/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Don't run this cell --- unless no data\n",
    "# downloading each report and saving it\n",
    "\n",
    "for index, row in cik_list.iterrows():\n",
    "    r = requests.get(base_url + row['SECFNAME'])\n",
    "    open('data/reports/' + row['SECFNAME'].split(os.sep)[3], 'wb').write(r.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Sentimental Analysis\n",
    "\n",
    "### 1.1 Cleaning using Stop Words Lists"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reading stopwords\n",
    "\n",
    "StopWords_Auditor = pd.read_csv('data/stopwords/StopWords_Auditor.txt', header = None)\n",
    "StopWords_Currencies = pd.read_fwf('data/stopwords/StopWords_Currencies.txt', header = None)\n",
    "StopWords_DatesandNumbers = pd.read_csv('data/stopwords/StopWords_DatesandNumbers.txt', header = None)\n",
    "StopWords_Generic = pd.read_csv('data/stopwords/StopWords_Generic.txt', header = None)\n",
    "StopWords_GenericLong = pd.read_csv('data/stopwords/StopWords_GenericLong.txt', header = None)\n",
    "StopWords_Geographic = pd.read_csv('data/stopwords/StopWords_Geographic.txt', header = None)\n",
    "StopWords_Names = pd.read_csv('data/stopwords/StopWords_Names.txt', header = None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ERNST</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>YOUNG</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>DELOITTE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>TOUCHE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>KPMG</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          0\n",
       "0     ERNST\n",
       "1     YOUNG\n",
       "2  DELOITTE\n",
       "3    TOUCHE\n",
       "4      KPMG"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "StopWords_Auditor.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Removing NA values from stopwords currencies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>76</th>\n",
       "      <td>SPECIAL DRAWING RIGHTS  | International Monetary</td>\n",
       "      <td>Fund</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                   0     1\n",
       "76  SPECIAL DRAWING RIGHTS  | International Monetary  Fund"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "StopWords_Currencies[~StopWords_Currencies[1].isna()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "StopWords_Currencies.iloc[76,1] = StopWords_Currencies.iloc[76,0] + \" \" + StopWords_Currencies.iloc[76,1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "StopWords_Currencies.drop(1, axis = 1, inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>AFGHANI  | Afghanistan</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ARIARY | Madagascar</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>BAHT | Thailand</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>BALBOA | Panama</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>BIRR | Ethiopia</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                        0\n",
       "0  AFGHANI  | Afghanistan\n",
       "1     ARIARY | Madagascar\n",
       "2         BAHT | Thailand\n",
       "3         BALBOA | Panama\n",
       "4         BIRR | Ethiopia"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "StopWords_Currencies.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Splitting pipe symbols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_pipe(x):\n",
    "    return pd.Series([x.split(\" | \")[0].strip(), x.split(\" | \")[1].strip()])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Splitting the words in column 0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "StopWords_Currencies[[0, 1]] = StopWords_Currencies[0].apply(lambda x: split_pipe(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>AFGHANI</td>\n",
       "      <td>Afghanistan</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ARIARY</td>\n",
       "      <td>Madagascar</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>BAHT</td>\n",
       "      <td>Thailand</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>BALBOA</td>\n",
       "      <td>Panama</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>BIRR</td>\n",
       "      <td>Ethiopia</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         0            1\n",
       "0  AFGHANI  Afghanistan\n",
       "1   ARIARY   Madagascar\n",
       "2     BAHT     Thailand\n",
       "3   BALBOA       Panama\n",
       "4     BIRR     Ethiopia"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "StopWords_Currencies.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Removing pipes in Dates and Numbers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_pipe(x):\n",
    "    return x.split(\" | \")[0].strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "StopWords_DatesandNumbers[0] = StopWords_DatesandNumbers[0].apply(lambda x: remove_pipe(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>HUNDRED</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>THOUSAND</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>MILLION</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>BILLION</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>TRILLION</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          0\n",
       "0   HUNDRED\n",
       "1  THOUSAND\n",
       "2   MILLION\n",
       "3   BILLION\n",
       "4  TRILLION"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "StopWords_DatesandNumbers.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ABOUT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ABOVE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>AFTER</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>AGAIN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ALL</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       0\n",
       "0  ABOUT\n",
       "1  ABOVE\n",
       "2  AFTER\n",
       "3  AGAIN\n",
       "4    ALL"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "StopWords_Generic.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>a</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>a's</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>able</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>about</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>above</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       0\n",
       "0      a\n",
       "1    a's\n",
       "2   able\n",
       "3  about\n",
       "4  above"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "StopWords_GenericLong.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Removing pipes in Geographic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "StopWords_Geographic[0] = StopWords_Geographic[0].apply(lambda x: remove_pipe(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>UNITED</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>STATE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>NORTH</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>SOUTH</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>EAST</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        0\n",
       "0  UNITED\n",
       "1   STATE\n",
       "2   NORTH\n",
       "3   SOUTH\n",
       "4    EAST"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "StopWords_Geographic.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Removing pipes from names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "StopWords_Names[0] = StopWords_Names[0].astype(str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "StopWords_Names[0] = StopWords_Names[0].apply(lambda x: remove_pipe(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>SMITH</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>JOHNSON</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>WILLIAMS</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>JONES</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>BROWN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          0\n",
       "0     SMITH\n",
       "1   JOHNSON\n",
       "2  WILLIAMS\n",
       "3     JONES\n",
       "4     BROWN"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "StopWords_Names.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# listing out all stop words\n",
    "stopwords = list(chain(StopWords_Currencies[0].to_list(),\n",
    "                       StopWords_Currencies[1].to_list(),\n",
    "                       StopWords_DatesandNumbers[0].to_list(),\n",
    "                       StopWords_Generic[0].to_list(),\n",
    "                       StopWords_GenericLong[0].to_list(),\n",
    "                       StopWords_Geographic[0].to_list(),\n",
    "                       StopWords_Names[0].to_list()))\n",
    "stopwords_dict = Counter(stopwords) # counter for faster execution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# iterating through each file, removing stop words and saving it\n",
    "\n",
    "for file in cik_list.SECFNAME:\n",
    "    f = open('data/reports/' + file.split(os.sep)[3], 'r').read()\n",
    "    f_clean = ' '.join([word for word in f.split() if word not in stopwords_dict])\n",
    "    open('data/reports_cleaned/' + file.split(os.sep)[3], 'w').write(f_clean)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 Creating dictionary of Positive and Negative words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "master_dictionary = pd.read_csv('data/LoughranMcDonald_MasterDictionary_2018.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Word</th>\n",
       "      <th>Sequence Number</th>\n",
       "      <th>Word Count</th>\n",
       "      <th>Word Proportion</th>\n",
       "      <th>Average Proportion</th>\n",
       "      <th>Std Dev</th>\n",
       "      <th>Doc Count</th>\n",
       "      <th>Negative</th>\n",
       "      <th>Positive</th>\n",
       "      <th>Uncertainty</th>\n",
       "      <th>Litigious</th>\n",
       "      <th>Constraining</th>\n",
       "      <th>Superfluous</th>\n",
       "      <th>Interesting</th>\n",
       "      <th>Modal</th>\n",
       "      <th>Irr_Verb</th>\n",
       "      <th>Harvard_IV</th>\n",
       "      <th>Syllables</th>\n",
       "      <th>Source</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>AARDVARK</td>\n",
       "      <td>1</td>\n",
       "      <td>277</td>\n",
       "      <td>1.480000e-08</td>\n",
       "      <td>1.240000e-08</td>\n",
       "      <td>3.560000e-06</td>\n",
       "      <td>84</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>12of12inf</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>AARDVARKS</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>1.600000e-10</td>\n",
       "      <td>9.730000e-12</td>\n",
       "      <td>9.860000e-09</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>12of12inf</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>ABACI</td>\n",
       "      <td>3</td>\n",
       "      <td>8</td>\n",
       "      <td>4.280000e-10</td>\n",
       "      <td>1.390000e-10</td>\n",
       "      <td>6.230000e-08</td>\n",
       "      <td>7</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>12of12inf</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ABACK</td>\n",
       "      <td>4</td>\n",
       "      <td>12</td>\n",
       "      <td>6.410000e-10</td>\n",
       "      <td>3.160000e-10</td>\n",
       "      <td>9.380000e-08</td>\n",
       "      <td>12</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>12of12inf</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ABACUS</td>\n",
       "      <td>5</td>\n",
       "      <td>7250</td>\n",
       "      <td>3.870000e-07</td>\n",
       "      <td>3.680000e-07</td>\n",
       "      <td>3.370000e-05</td>\n",
       "      <td>914</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>12of12inf</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        Word  Sequence Number  Word Count  Word Proportion  \\\n",
       "0   AARDVARK                1         277     1.480000e-08   \n",
       "1  AARDVARKS                2           3     1.600000e-10   \n",
       "2      ABACI                3           8     4.280000e-10   \n",
       "3      ABACK                4          12     6.410000e-10   \n",
       "4     ABACUS                5        7250     3.870000e-07   \n",
       "\n",
       "   Average Proportion       Std Dev  Doc Count  Negative  Positive  \\\n",
       "0        1.240000e-08  3.560000e-06         84         0         0   \n",
       "1        9.730000e-12  9.860000e-09          1         0         0   \n",
       "2        1.390000e-10  6.230000e-08          7         0         0   \n",
       "3        3.160000e-10  9.380000e-08         12         0         0   \n",
       "4        3.680000e-07  3.370000e-05        914         0         0   \n",
       "\n",
       "   Uncertainty  Litigious  Constraining  Superfluous  Interesting  Modal  \\\n",
       "0            0          0             0            0            0      0   \n",
       "1            0          0             0            0            0      0   \n",
       "2            0          0             0            0            0      0   \n",
       "3            0          0             0            0            0      0   \n",
       "4            0          0             0            0            0      0   \n",
       "\n",
       "   Irr_Verb  Harvard_IV  Syllables     Source  \n",
       "0         0           0          2  12of12inf  \n",
       "1         0           0          2  12of12inf  \n",
       "2         0           0          3  12of12inf  \n",
       "3         0           0          2  12of12inf  \n",
       "4         0           0          3  12of12inf  "
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "master_dictionary.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "negative_dict = Counter(master_dictionary['Word'][master_dictionary.Negative > 0].to_list())\n",
    "positive_dict = Counter(master_dictionary['Word'][master_dictionary.Positive > 0].to_list())\n",
    "negative_dict = Counter(word for word in negative_dict if word not in stopwords_dict)\n",
    "positive_dict = Counter(word for word in positive_dict if word not in stopwords_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3 Extracting Derived variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# iterating through each file\n",
    "\n",
    "for i, row in cik_list.iterrows():\n",
    "    f = open('data/reports_cleaned/' + row['SECFNAME'].split(os.sep)[3], 'r').read()\n",
    "    doc_dict = Counter(word_tokenize(f))\n",
    "    \n",
    "    # getting positive score\n",
    "    positive_f_dict = doc_dict & positive_dict\n",
    "    cik_list.loc[i, 'positive_score'] = sum(positive_f_dict.values())\n",
    "    \n",
    "    # getting negative score\n",
    "    negative_f_dict = doc_dict & negative_dict\n",
    "    cik_list.loc[i, 'negative_score'] = sum(negative_f_dict.values())\n",
    "    \n",
    "    # total words after cleaning\n",
    "    cik_list.loc[i, 'total_words_after_cleaning'] = sum(doc_dict.values())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "cik_list['polarity_score'] = cik_list\\\n",
    ".apply(lambda x: (x['positive_score'] - x['negative_score'])/((x['positive_score'] + x['negative_score'])+0.000001),\n",
    "      axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "cik_list['subjectivity_score'] = cik_list\\\n",
    ".apply(lambda x: (x['positive_score'] + x['negative_score'])/(x['total_words_after_cleaning'] + 0.000001), \n",
    "       axis = 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.4 Sentiment score categorization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sentiment_score_categorization(x):\n",
    "    if x <= -0.5:\n",
    "        return \"most_negative\"\n",
    "    elif x > -0.5 and x < 0:\n",
    "        return \"negative\"\n",
    "    elif x == 0:\n",
    "        return \"negative\"\n",
    "    elif x > 0 and x < 0.5:\n",
    "        return \"positive\"\n",
    "    else:\n",
    "        return \"very_positive\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "cik_list['sentiment_score_categorization'] = cik_list.polarity_score.apply(sentiment_score_categorization)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>CIK</th>\n",
       "      <th>CONAME</th>\n",
       "      <th>FYRMO</th>\n",
       "      <th>FDATE</th>\n",
       "      <th>FORM</th>\n",
       "      <th>SECFNAME</th>\n",
       "      <th>positive_score</th>\n",
       "      <th>negative_score</th>\n",
       "      <th>total_words_after_cleaning</th>\n",
       "      <th>polarity_score</th>\n",
       "      <th>subjectivity_score</th>\n",
       "      <th>sentiment_score_categorization</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>3662</td>\n",
       "      <td>SUNBEAM CORP/FL/</td>\n",
       "      <td>199803</td>\n",
       "      <td>1998-03-06</td>\n",
       "      <td>10-K405</td>\n",
       "      <td>edgar/data/3662/0000950170-98-000413.txt</td>\n",
       "      <td>6.0</td>\n",
       "      <td>39.0</td>\n",
       "      <td>126872.0</td>\n",
       "      <td>-0.733333</td>\n",
       "      <td>0.000355</td>\n",
       "      <td>most_negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>3662</td>\n",
       "      <td>SUNBEAM CORP/FL/</td>\n",
       "      <td>199805</td>\n",
       "      <td>1998-05-15</td>\n",
       "      <td>10-Q</td>\n",
       "      <td>edgar/data/3662/0000950170-98-001001.txt</td>\n",
       "      <td>3.0</td>\n",
       "      <td>34.0</td>\n",
       "      <td>79339.0</td>\n",
       "      <td>-0.837838</td>\n",
       "      <td>0.000466</td>\n",
       "      <td>most_negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3662</td>\n",
       "      <td>SUNBEAM CORP/FL/</td>\n",
       "      <td>199808</td>\n",
       "      <td>1998-08-13</td>\n",
       "      <td>NT 10-Q</td>\n",
       "      <td>edgar/data/3662/0000950172-98-000783.txt</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>881.0</td>\n",
       "      <td>-0.999999</td>\n",
       "      <td>0.001135</td>\n",
       "      <td>most_negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3662</td>\n",
       "      <td>SUNBEAM CORP/FL/</td>\n",
       "      <td>199811</td>\n",
       "      <td>1998-11-12</td>\n",
       "      <td>10-K/A</td>\n",
       "      <td>edgar/data/3662/0000950170-98-002145.txt</td>\n",
       "      <td>5.0</td>\n",
       "      <td>32.0</td>\n",
       "      <td>69772.0</td>\n",
       "      <td>-0.729730</td>\n",
       "      <td>0.000530</td>\n",
       "      <td>most_negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>3662</td>\n",
       "      <td>SUNBEAM CORP/FL/</td>\n",
       "      <td>199811</td>\n",
       "      <td>1998-11-16</td>\n",
       "      <td>NT 10-Q</td>\n",
       "      <td>edgar/data/3662/0000950172-98-001203.txt</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1072.0</td>\n",
       "      <td>-0.999999</td>\n",
       "      <td>0.000933</td>\n",
       "      <td>most_negative</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    CIK            CONAME   FYRMO      FDATE     FORM  \\\n",
       "0  3662  SUNBEAM CORP/FL/  199803 1998-03-06  10-K405   \n",
       "1  3662  SUNBEAM CORP/FL/  199805 1998-05-15     10-Q   \n",
       "2  3662  SUNBEAM CORP/FL/  199808 1998-08-13  NT 10-Q   \n",
       "3  3662  SUNBEAM CORP/FL/  199811 1998-11-12   10-K/A   \n",
       "4  3662  SUNBEAM CORP/FL/  199811 1998-11-16  NT 10-Q   \n",
       "\n",
       "                                   SECFNAME  positive_score  negative_score  \\\n",
       "0  edgar/data/3662/0000950170-98-000413.txt             6.0            39.0   \n",
       "1  edgar/data/3662/0000950170-98-001001.txt             3.0            34.0   \n",
       "2  edgar/data/3662/0000950172-98-000783.txt             0.0             1.0   \n",
       "3  edgar/data/3662/0000950170-98-002145.txt             5.0            32.0   \n",
       "4  edgar/data/3662/0000950172-98-001203.txt             0.0             1.0   \n",
       "\n",
       "   total_words_after_cleaning  polarity_score  subjectivity_score  \\\n",
       "0                    126872.0       -0.733333            0.000355   \n",
       "1                     79339.0       -0.837838            0.000466   \n",
       "2                       881.0       -0.999999            0.001135   \n",
       "3                     69772.0       -0.729730            0.000530   \n",
       "4                      1072.0       -0.999999            0.000933   \n",
       "\n",
       "  sentiment_score_categorization  \n",
       "0                  most_negative  \n",
       "1                  most_negative  \n",
       "2                  most_negative  \n",
       "3                  most_negative  \n",
       "4                  most_negative  "
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cik_list.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Analysis of Readability - Gunning Fox index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# getting number of sentences\n",
    "for i, row in cik_list.iterrows():\n",
    "    f = open('data/reports_cleaned/' + row['SECFNAME'].split(os.sep)[3], 'r').read()\n",
    "    doc_dict = Counter(sent_tokenize(f))\n",
    "\n",
    "    # total words after cleaning\n",
    "    cik_list.loc[i, 'total_sentences_after_cleaning'] = sum(doc_dict.values())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "cik_list['average_sentence_length'] = cik_list\\\n",
    ".apply(lambda x: x['total_words_after_cleaning']/x['total_sentences_after_cleaning'], axis = 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Getting complex words\n",
    "\n",
    "Anything word with more than two syllables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "dic = pyphen.Pyphen(lang='en')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, row in cik_list.iterrows():\n",
    "    f = open('data/reports_cleaned/' + row['SECFNAME'].split(os.sep)[3], 'r').read()\n",
    "    doc_dict = Counter(word_tokenize(f))\n",
    "    \n",
    "    complex_words = (1 for word in doc_dict if len(dic.inserted(word)) > 2)\n",
    "    cik_list.loc[i, 'complex_word_count'] = sum(complex_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "cik_list['percentage_of_complex_words'] = cik_list['complex_word_count']/cik_list['total_words_after_cleaning']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "cik_list['fog_index'] = 0.4 * (cik_list['average_sentence_length'] + cik_list['percentage_of_complex_words'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>CIK</th>\n",
       "      <th>CONAME</th>\n",
       "      <th>FYRMO</th>\n",
       "      <th>FDATE</th>\n",
       "      <th>FORM</th>\n",
       "      <th>SECFNAME</th>\n",
       "      <th>positive_score</th>\n",
       "      <th>negative_score</th>\n",
       "      <th>total_words_after_cleaning</th>\n",
       "      <th>polarity_score</th>\n",
       "      <th>subjectivity_score</th>\n",
       "      <th>sentiment_score_categorization</th>\n",
       "      <th>total_sentences_after_cleaning</th>\n",
       "      <th>average_sentence_length</th>\n",
       "      <th>complex_word_count</th>\n",
       "      <th>percentage_of_complex_words</th>\n",
       "      <th>fog_index</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>3662</td>\n",
       "      <td>SUNBEAM CORP/FL/</td>\n",
       "      <td>199803</td>\n",
       "      <td>1998-03-06</td>\n",
       "      <td>10-K405</td>\n",
       "      <td>edgar/data/3662/0000950170-98-000413.txt</td>\n",
       "      <td>6.0</td>\n",
       "      <td>39.0</td>\n",
       "      <td>126872.0</td>\n",
       "      <td>-0.733333</td>\n",
       "      <td>0.000355</td>\n",
       "      <td>most_negative</td>\n",
       "      <td>3975.0</td>\n",
       "      <td>31.917484</td>\n",
       "      <td>7889.0</td>\n",
       "      <td>0.062181</td>\n",
       "      <td>12.791866</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>3662</td>\n",
       "      <td>SUNBEAM CORP/FL/</td>\n",
       "      <td>199805</td>\n",
       "      <td>1998-05-15</td>\n",
       "      <td>10-Q</td>\n",
       "      <td>edgar/data/3662/0000950170-98-001001.txt</td>\n",
       "      <td>3.0</td>\n",
       "      <td>34.0</td>\n",
       "      <td>79339.0</td>\n",
       "      <td>-0.837838</td>\n",
       "      <td>0.000466</td>\n",
       "      <td>most_negative</td>\n",
       "      <td>2499.0</td>\n",
       "      <td>31.748299</td>\n",
       "      <td>5880.0</td>\n",
       "      <td>0.074112</td>\n",
       "      <td>12.728965</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3662</td>\n",
       "      <td>SUNBEAM CORP/FL/</td>\n",
       "      <td>199808</td>\n",
       "      <td>1998-08-13</td>\n",
       "      <td>NT 10-Q</td>\n",
       "      <td>edgar/data/3662/0000950172-98-000783.txt</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>881.0</td>\n",
       "      <td>-0.999999</td>\n",
       "      <td>0.001135</td>\n",
       "      <td>most_negative</td>\n",
       "      <td>20.0</td>\n",
       "      <td>44.050000</td>\n",
       "      <td>334.0</td>\n",
       "      <td>0.379115</td>\n",
       "      <td>17.771646</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3662</td>\n",
       "      <td>SUNBEAM CORP/FL/</td>\n",
       "      <td>199811</td>\n",
       "      <td>1998-11-12</td>\n",
       "      <td>10-K/A</td>\n",
       "      <td>edgar/data/3662/0000950170-98-002145.txt</td>\n",
       "      <td>5.0</td>\n",
       "      <td>32.0</td>\n",
       "      <td>69772.0</td>\n",
       "      <td>-0.729730</td>\n",
       "      <td>0.000530</td>\n",
       "      <td>most_negative</td>\n",
       "      <td>2487.0</td>\n",
       "      <td>28.054684</td>\n",
       "      <td>6833.0</td>\n",
       "      <td>0.097933</td>\n",
       "      <td>11.261047</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>3662</td>\n",
       "      <td>SUNBEAM CORP/FL/</td>\n",
       "      <td>199811</td>\n",
       "      <td>1998-11-16</td>\n",
       "      <td>NT 10-Q</td>\n",
       "      <td>edgar/data/3662/0000950172-98-001203.txt</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1072.0</td>\n",
       "      <td>-0.999999</td>\n",
       "      <td>0.000933</td>\n",
       "      <td>most_negative</td>\n",
       "      <td>25.0</td>\n",
       "      <td>42.880000</td>\n",
       "      <td>358.0</td>\n",
       "      <td>0.333955</td>\n",
       "      <td>17.285582</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    CIK            CONAME   FYRMO      FDATE     FORM  \\\n",
       "0  3662  SUNBEAM CORP/FL/  199803 1998-03-06  10-K405   \n",
       "1  3662  SUNBEAM CORP/FL/  199805 1998-05-15     10-Q   \n",
       "2  3662  SUNBEAM CORP/FL/  199808 1998-08-13  NT 10-Q   \n",
       "3  3662  SUNBEAM CORP/FL/  199811 1998-11-12   10-K/A   \n",
       "4  3662  SUNBEAM CORP/FL/  199811 1998-11-16  NT 10-Q   \n",
       "\n",
       "                                   SECFNAME  positive_score  negative_score  \\\n",
       "0  edgar/data/3662/0000950170-98-000413.txt             6.0            39.0   \n",
       "1  edgar/data/3662/0000950170-98-001001.txt             3.0            34.0   \n",
       "2  edgar/data/3662/0000950172-98-000783.txt             0.0             1.0   \n",
       "3  edgar/data/3662/0000950170-98-002145.txt             5.0            32.0   \n",
       "4  edgar/data/3662/0000950172-98-001203.txt             0.0             1.0   \n",
       "\n",
       "   total_words_after_cleaning  polarity_score  subjectivity_score  \\\n",
       "0                    126872.0       -0.733333            0.000355   \n",
       "1                     79339.0       -0.837838            0.000466   \n",
       "2                       881.0       -0.999999            0.001135   \n",
       "3                     69772.0       -0.729730            0.000530   \n",
       "4                      1072.0       -0.999999            0.000933   \n",
       "\n",
       "  sentiment_score_categorization  total_sentences_after_cleaning  \\\n",
       "0                  most_negative                          3975.0   \n",
       "1                  most_negative                          2499.0   \n",
       "2                  most_negative                            20.0   \n",
       "3                  most_negative                          2487.0   \n",
       "4                  most_negative                            25.0   \n",
       "\n",
       "   average_sentence_length  complex_word_count  percentage_of_complex_words  \\\n",
       "0                31.917484              7889.0                     0.062181   \n",
       "1                31.748299              5880.0                     0.074112   \n",
       "2                44.050000               334.0                     0.379115   \n",
       "3                28.054684              6833.0                     0.097933   \n",
       "4                42.880000               358.0                     0.333955   \n",
       "\n",
       "   fog_index  \n",
       "0  12.791866  \n",
       "1  12.728965  \n",
       "2  17.771646  \n",
       "3  11.261047  \n",
       "4  17.285582  "
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cik_list.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Word Count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "cachedStopWords = nltk_stopwords.words(\"english\")\n",
    "stopwords_dict_nltk = Counter(cachedStopWords)\n",
    "punctuations = '''!()-[]{};:'\"\\,<>./?@#$%^&*_~'''\n",
    "punctuation_dict = Counter(punctuations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, row in cik_list.iterrows():\n",
    "    f = open('data/reports_cleaned/' + row['SECFNAME'].split(os.sep)[3], 'r').read()\n",
    "    doc_dict = Counter(word_tokenize(f))\n",
    "    \n",
    "    # removing stop words and counting words\n",
    "    word_count = (1 for word in doc_dict if word not in stopwords_dict_nltk and word not in punctuation_dict)\n",
    "    cik_list.loc[i, 'word_count'] = sum(word_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>CIK</th>\n",
       "      <th>CONAME</th>\n",
       "      <th>FYRMO</th>\n",
       "      <th>FDATE</th>\n",
       "      <th>FORM</th>\n",
       "      <th>SECFNAME</th>\n",
       "      <th>positive_score</th>\n",
       "      <th>negative_score</th>\n",
       "      <th>total_words_after_cleaning</th>\n",
       "      <th>polarity_score</th>\n",
       "      <th>subjectivity_score</th>\n",
       "      <th>sentiment_score_categorization</th>\n",
       "      <th>total_sentences_after_cleaning</th>\n",
       "      <th>average_sentence_length</th>\n",
       "      <th>complex_word_count</th>\n",
       "      <th>percentage_of_complex_words</th>\n",
       "      <th>fog_index</th>\n",
       "      <th>word_count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>3662</td>\n",
       "      <td>SUNBEAM CORP/FL/</td>\n",
       "      <td>199803</td>\n",
       "      <td>1998-03-06</td>\n",
       "      <td>10-K405</td>\n",
       "      <td>edgar/data/3662/0000950170-98-000413.txt</td>\n",
       "      <td>6.0</td>\n",
       "      <td>39.0</td>\n",
       "      <td>126872.0</td>\n",
       "      <td>-0.733333</td>\n",
       "      <td>0.000355</td>\n",
       "      <td>most_negative</td>\n",
       "      <td>3975.0</td>\n",
       "      <td>31.917484</td>\n",
       "      <td>7889.0</td>\n",
       "      <td>0.062181</td>\n",
       "      <td>12.791866</td>\n",
       "      <td>8052.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>3662</td>\n",
       "      <td>SUNBEAM CORP/FL/</td>\n",
       "      <td>199805</td>\n",
       "      <td>1998-05-15</td>\n",
       "      <td>10-Q</td>\n",
       "      <td>edgar/data/3662/0000950170-98-001001.txt</td>\n",
       "      <td>3.0</td>\n",
       "      <td>34.0</td>\n",
       "      <td>79339.0</td>\n",
       "      <td>-0.837838</td>\n",
       "      <td>0.000466</td>\n",
       "      <td>most_negative</td>\n",
       "      <td>2499.0</td>\n",
       "      <td>31.748299</td>\n",
       "      <td>5880.0</td>\n",
       "      <td>0.074112</td>\n",
       "      <td>12.728965</td>\n",
       "      <td>6031.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3662</td>\n",
       "      <td>SUNBEAM CORP/FL/</td>\n",
       "      <td>199808</td>\n",
       "      <td>1998-08-13</td>\n",
       "      <td>NT 10-Q</td>\n",
       "      <td>edgar/data/3662/0000950172-98-000783.txt</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>881.0</td>\n",
       "      <td>-0.999999</td>\n",
       "      <td>0.001135</td>\n",
       "      <td>most_negative</td>\n",
       "      <td>20.0</td>\n",
       "      <td>44.050000</td>\n",
       "      <td>334.0</td>\n",
       "      <td>0.379115</td>\n",
       "      <td>17.771646</td>\n",
       "      <td>358.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3662</td>\n",
       "      <td>SUNBEAM CORP/FL/</td>\n",
       "      <td>199811</td>\n",
       "      <td>1998-11-12</td>\n",
       "      <td>10-K/A</td>\n",
       "      <td>edgar/data/3662/0000950170-98-002145.txt</td>\n",
       "      <td>5.0</td>\n",
       "      <td>32.0</td>\n",
       "      <td>69772.0</td>\n",
       "      <td>-0.729730</td>\n",
       "      <td>0.000530</td>\n",
       "      <td>most_negative</td>\n",
       "      <td>2487.0</td>\n",
       "      <td>28.054684</td>\n",
       "      <td>6833.0</td>\n",
       "      <td>0.097933</td>\n",
       "      <td>11.261047</td>\n",
       "      <td>6981.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>3662</td>\n",
       "      <td>SUNBEAM CORP/FL/</td>\n",
       "      <td>199811</td>\n",
       "      <td>1998-11-16</td>\n",
       "      <td>NT 10-Q</td>\n",
       "      <td>edgar/data/3662/0000950172-98-001203.txt</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1072.0</td>\n",
       "      <td>-0.999999</td>\n",
       "      <td>0.000933</td>\n",
       "      <td>most_negative</td>\n",
       "      <td>25.0</td>\n",
       "      <td>42.880000</td>\n",
       "      <td>358.0</td>\n",
       "      <td>0.333955</td>\n",
       "      <td>17.285582</td>\n",
       "      <td>383.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    CIK            CONAME   FYRMO      FDATE     FORM  \\\n",
       "0  3662  SUNBEAM CORP/FL/  199803 1998-03-06  10-K405   \n",
       "1  3662  SUNBEAM CORP/FL/  199805 1998-05-15     10-Q   \n",
       "2  3662  SUNBEAM CORP/FL/  199808 1998-08-13  NT 10-Q   \n",
       "3  3662  SUNBEAM CORP/FL/  199811 1998-11-12   10-K/A   \n",
       "4  3662  SUNBEAM CORP/FL/  199811 1998-11-16  NT 10-Q   \n",
       "\n",
       "                                   SECFNAME  positive_score  negative_score  \\\n",
       "0  edgar/data/3662/0000950170-98-000413.txt             6.0            39.0   \n",
       "1  edgar/data/3662/0000950170-98-001001.txt             3.0            34.0   \n",
       "2  edgar/data/3662/0000950172-98-000783.txt             0.0             1.0   \n",
       "3  edgar/data/3662/0000950170-98-002145.txt             5.0            32.0   \n",
       "4  edgar/data/3662/0000950172-98-001203.txt             0.0             1.0   \n",
       "\n",
       "   total_words_after_cleaning  polarity_score  subjectivity_score  \\\n",
       "0                    126872.0       -0.733333            0.000355   \n",
       "1                     79339.0       -0.837838            0.000466   \n",
       "2                       881.0       -0.999999            0.001135   \n",
       "3                     69772.0       -0.729730            0.000530   \n",
       "4                      1072.0       -0.999999            0.000933   \n",
       "\n",
       "  sentiment_score_categorization  total_sentences_after_cleaning  \\\n",
       "0                  most_negative                          3975.0   \n",
       "1                  most_negative                          2499.0   \n",
       "2                  most_negative                            20.0   \n",
       "3                  most_negative                          2487.0   \n",
       "4                  most_negative                            25.0   \n",
       "\n",
       "   average_sentence_length  complex_word_count  percentage_of_complex_words  \\\n",
       "0                31.917484              7889.0                     0.062181   \n",
       "1                31.748299              5880.0                     0.074112   \n",
       "2                44.050000               334.0                     0.379115   \n",
       "3                28.054684              6833.0                     0.097933   \n",
       "4                42.880000               358.0                     0.333955   \n",
       "\n",
       "   fog_index  word_count  \n",
       "0  12.791866      8052.0  \n",
       "1  12.728965      6031.0  \n",
       "2  17.771646       358.0  \n",
       "3  11.261047      6981.0  \n",
       "4  17.285582       383.0  "
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cik_list.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Uncertainty and constraining"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "uncertainty_dictionary = pd.read_excel('data/uncertainty_dictionary.xlsx')\n",
    "constraining_dictionary = pd.read_excel('data/constraining_dictionary.xlsx')\n",
    "uncertainty_dict = Counter(uncertainty_dictionary['Word'].to_list())\n",
    "constraining_dict = Counter(constraining_dictionary['Word'].to_list())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, row in cik_list.iterrows():\n",
    "    f = open('data/reports_cleaned/' + row['SECFNAME'].split(os.sep)[3], 'r').read()\n",
    "    doc_dict = Counter(word_tokenize(f))\n",
    "    \n",
    "    # getting uncertainity\n",
    "    uncertainty_f_dict = doc_dict & uncertainty_dict\n",
    "    cik_list.loc[i, 'uncertainty'] = sum(uncertainty_f_dict.values())\n",
    "    \n",
    "    # getting constraining\n",
    "    constraining_f_dict = doc_dict & constraining_dict\n",
    "    cik_list.loc[i, 'constraining'] = sum(constraining_f_dict.values())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>CIK</th>\n",
       "      <td>3662</td>\n",
       "      <td>3662</td>\n",
       "      <td>3662</td>\n",
       "      <td>3662</td>\n",
       "      <td>3662</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>CONAME</th>\n",
       "      <td>SUNBEAM CORP/FL/</td>\n",
       "      <td>SUNBEAM CORP/FL/</td>\n",
       "      <td>SUNBEAM CORP/FL/</td>\n",
       "      <td>SUNBEAM CORP/FL/</td>\n",
       "      <td>SUNBEAM CORP/FL/</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>FYRMO</th>\n",
       "      <td>199803</td>\n",
       "      <td>199805</td>\n",
       "      <td>199808</td>\n",
       "      <td>199811</td>\n",
       "      <td>199811</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>FDATE</th>\n",
       "      <td>1998-03-06 00:00:00</td>\n",
       "      <td>1998-05-15 00:00:00</td>\n",
       "      <td>1998-08-13 00:00:00</td>\n",
       "      <td>1998-11-12 00:00:00</td>\n",
       "      <td>1998-11-16 00:00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>FORM</th>\n",
       "      <td>10-K405</td>\n",
       "      <td>10-Q</td>\n",
       "      <td>NT 10-Q</td>\n",
       "      <td>10-K/A</td>\n",
       "      <td>NT 10-Q</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>SECFNAME</th>\n",
       "      <td>edgar/data/3662/0000950170-98-000413.txt</td>\n",
       "      <td>edgar/data/3662/0000950170-98-001001.txt</td>\n",
       "      <td>edgar/data/3662/0000950172-98-000783.txt</td>\n",
       "      <td>edgar/data/3662/0000950170-98-002145.txt</td>\n",
       "      <td>edgar/data/3662/0000950172-98-001203.txt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>positive_score</th>\n",
       "      <td>6</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>negative_score</th>\n",
       "      <td>39</td>\n",
       "      <td>34</td>\n",
       "      <td>1</td>\n",
       "      <td>32</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>total_words_after_cleaning</th>\n",
       "      <td>126872</td>\n",
       "      <td>79339</td>\n",
       "      <td>881</td>\n",
       "      <td>69772</td>\n",
       "      <td>1072</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>polarity_score</th>\n",
       "      <td>-0.733333</td>\n",
       "      <td>-0.837838</td>\n",
       "      <td>-0.999999</td>\n",
       "      <td>-0.72973</td>\n",
       "      <td>-0.999999</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>subjectivity_score</th>\n",
       "      <td>0.000354688</td>\n",
       "      <td>0.000466353</td>\n",
       "      <td>0.00113507</td>\n",
       "      <td>0.000530299</td>\n",
       "      <td>0.000932836</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>sentiment_score_categorization</th>\n",
       "      <td>most_negative</td>\n",
       "      <td>most_negative</td>\n",
       "      <td>most_negative</td>\n",
       "      <td>most_negative</td>\n",
       "      <td>most_negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>total_sentences_after_cleaning</th>\n",
       "      <td>3975</td>\n",
       "      <td>2499</td>\n",
       "      <td>20</td>\n",
       "      <td>2487</td>\n",
       "      <td>25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>average_sentence_length</th>\n",
       "      <td>31.9175</td>\n",
       "      <td>31.7483</td>\n",
       "      <td>44.05</td>\n",
       "      <td>28.0547</td>\n",
       "      <td>42.88</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>complex_word_count</th>\n",
       "      <td>7889</td>\n",
       "      <td>5880</td>\n",
       "      <td>334</td>\n",
       "      <td>6833</td>\n",
       "      <td>358</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>percentage_of_complex_words</th>\n",
       "      <td>0.0621808</td>\n",
       "      <td>0.0741124</td>\n",
       "      <td>0.379115</td>\n",
       "      <td>0.0979333</td>\n",
       "      <td>0.333955</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>fog_index</th>\n",
       "      <td>12.7919</td>\n",
       "      <td>12.729</td>\n",
       "      <td>17.7716</td>\n",
       "      <td>11.261</td>\n",
       "      <td>17.2856</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>word_count</th>\n",
       "      <td>8052</td>\n",
       "      <td>6031</td>\n",
       "      <td>358</td>\n",
       "      <td>6981</td>\n",
       "      <td>383</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>uncertainty</th>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>7</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>constraining</th>\n",
       "      <td>17</td>\n",
       "      <td>17</td>\n",
       "      <td>0</td>\n",
       "      <td>16</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                       0  \\\n",
       "CIK                                                                 3662   \n",
       "CONAME                                                  SUNBEAM CORP/FL/   \n",
       "FYRMO                                                             199803   \n",
       "FDATE                                                1998-03-06 00:00:00   \n",
       "FORM                                                             10-K405   \n",
       "SECFNAME                        edgar/data/3662/0000950170-98-000413.txt   \n",
       "positive_score                                                         6   \n",
       "negative_score                                                        39   \n",
       "total_words_after_cleaning                                        126872   \n",
       "polarity_score                                                 -0.733333   \n",
       "subjectivity_score                                           0.000354688   \n",
       "sentiment_score_categorization                             most_negative   \n",
       "total_sentences_after_cleaning                                      3975   \n",
       "average_sentence_length                                          31.9175   \n",
       "complex_word_count                                                  7889   \n",
       "percentage_of_complex_words                                    0.0621808   \n",
       "fog_index                                                        12.7919   \n",
       "word_count                                                          8052   \n",
       "uncertainty                                                            4   \n",
       "constraining                                                          17   \n",
       "\n",
       "                                                                       1  \\\n",
       "CIK                                                                 3662   \n",
       "CONAME                                                  SUNBEAM CORP/FL/   \n",
       "FYRMO                                                             199805   \n",
       "FDATE                                                1998-05-15 00:00:00   \n",
       "FORM                                                                10-Q   \n",
       "SECFNAME                        edgar/data/3662/0000950170-98-001001.txt   \n",
       "positive_score                                                         3   \n",
       "negative_score                                                        34   \n",
       "total_words_after_cleaning                                         79339   \n",
       "polarity_score                                                 -0.837838   \n",
       "subjectivity_score                                           0.000466353   \n",
       "sentiment_score_categorization                             most_negative   \n",
       "total_sentences_after_cleaning                                      2499   \n",
       "average_sentence_length                                          31.7483   \n",
       "complex_word_count                                                  5880   \n",
       "percentage_of_complex_words                                    0.0741124   \n",
       "fog_index                                                         12.729   \n",
       "word_count                                                          6031   \n",
       "uncertainty                                                            1   \n",
       "constraining                                                          17   \n",
       "\n",
       "                                                                       2  \\\n",
       "CIK                                                                 3662   \n",
       "CONAME                                                  SUNBEAM CORP/FL/   \n",
       "FYRMO                                                             199808   \n",
       "FDATE                                                1998-08-13 00:00:00   \n",
       "FORM                                                             NT 10-Q   \n",
       "SECFNAME                        edgar/data/3662/0000950172-98-000783.txt   \n",
       "positive_score                                                         0   \n",
       "negative_score                                                         1   \n",
       "total_words_after_cleaning                                           881   \n",
       "polarity_score                                                 -0.999999   \n",
       "subjectivity_score                                            0.00113507   \n",
       "sentiment_score_categorization                             most_negative   \n",
       "total_sentences_after_cleaning                                        20   \n",
       "average_sentence_length                                            44.05   \n",
       "complex_word_count                                                   334   \n",
       "percentage_of_complex_words                                     0.379115   \n",
       "fog_index                                                        17.7716   \n",
       "word_count                                                           358   \n",
       "uncertainty                                                            0   \n",
       "constraining                                                           0   \n",
       "\n",
       "                                                                       3  \\\n",
       "CIK                                                                 3662   \n",
       "CONAME                                                  SUNBEAM CORP/FL/   \n",
       "FYRMO                                                             199811   \n",
       "FDATE                                                1998-11-12 00:00:00   \n",
       "FORM                                                              10-K/A   \n",
       "SECFNAME                        edgar/data/3662/0000950170-98-002145.txt   \n",
       "positive_score                                                         5   \n",
       "negative_score                                                        32   \n",
       "total_words_after_cleaning                                         69772   \n",
       "polarity_score                                                  -0.72973   \n",
       "subjectivity_score                                           0.000530299   \n",
       "sentiment_score_categorization                             most_negative   \n",
       "total_sentences_after_cleaning                                      2487   \n",
       "average_sentence_length                                          28.0547   \n",
       "complex_word_count                                                  6833   \n",
       "percentage_of_complex_words                                    0.0979333   \n",
       "fog_index                                                         11.261   \n",
       "word_count                                                          6981   \n",
       "uncertainty                                                            7   \n",
       "constraining                                                          16   \n",
       "\n",
       "                                                                       4  \n",
       "CIK                                                                 3662  \n",
       "CONAME                                                  SUNBEAM CORP/FL/  \n",
       "FYRMO                                                             199811  \n",
       "FDATE                                                1998-11-16 00:00:00  \n",
       "FORM                                                             NT 10-Q  \n",
       "SECFNAME                        edgar/data/3662/0000950172-98-001203.txt  \n",
       "positive_score                                                         0  \n",
       "negative_score                                                         1  \n",
       "total_words_after_cleaning                                          1072  \n",
       "polarity_score                                                 -0.999999  \n",
       "subjectivity_score                                           0.000932836  \n",
       "sentiment_score_categorization                             most_negative  \n",
       "total_sentences_after_cleaning                                        25  \n",
       "average_sentence_length                                            42.88  \n",
       "complex_word_count                                                   358  \n",
       "percentage_of_complex_words                                     0.333955  \n",
       "fog_index                                                        17.2856  \n",
       "word_count                                                           383  \n",
       "uncertainty                                                            0  \n",
       "constraining                                                           0  "
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cik_list.head(5).T"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sections"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dividing each report into its three sections\n",
    "\n",
    "for i, row in cik_list.iterrows():\n",
    "    f = open('data/reports/' + row['SECFNAME'].split(os.sep)[3], 'r')\n",
    "    \n",
    "    page = 0\n",
    "    previous_line = \"\"\n",
    "    mda_flag = False\n",
    "    mda = \"\"\n",
    "    qqdmr_flag = False\n",
    "    qqdmr = \"\"\n",
    "    rf_flag = False\n",
    "    rf = \"\"\n",
    "    \n",
    "    for line in f:\n",
    "        if mda_flag and 'ITEM' in line:\n",
    "            mda_flag = False\n",
    "        if mda_flag:\n",
    "            mda = \" \".join((mda, line))\n",
    "        if qqdmr_flag and 'ITEM' in line:\n",
    "            qqdmr_flag = False\n",
    "        if qqdmr_flag:\n",
    "            qqdmr = \" \".join((qqdmr, line))\n",
    "        if rf_flag and 'ITEM' in line:\n",
    "            rf_flag = False\n",
    "        if rf_flag:\n",
    "            rf = \" \".join((rf, line))\n",
    "        if \"PAGE\" in line:\n",
    "            try:\n",
    "                page = int(re.findall(r'\\d+', previous_line)[0])\n",
    "            except:\n",
    "                continue\n",
    "        if \"MANAGEMENT'S DISCUSSION AND ANALYSIS\" in line and page >= 1:\n",
    "            mda_flag = True\n",
    "        if \"QUANTITATIVE AND QUALITATIVE DISCLOSURES ABOUT MARKET RISK\" in line and page >= 1:\n",
    "            qqdmr_flag = True\n",
    "        if \"RISK FACTORS\" in line and page >= 1:\n",
    "            rf_flag = True\n",
    "        previous_line = line\n",
    "    \n",
    "    open('data/reports/mda/' + row['SECFNAME'].split(os.sep)[3], 'w').write(mda)\n",
    "    open('data/reports/qqdmr/' + row['SECFNAME'].split(os.sep)[3], 'w').write(qqdmr)\n",
    "    open('data/reports/rf/' + row['SECFNAME'].split(os.sep)[3], 'w').write(rf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Getting Scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "cik_list = pd.read_excel('data/cik_list.xlsx')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "# iterating through each file, removing stop words and saving it\n",
    "\n",
    "for file in cik_list.SECFNAME:\n",
    "    f = open('data/reports/mda/' + file.split(os.sep)[3], 'r').read()\n",
    "    f_clean = ' '.join([word for word in f.split() if word not in stopwords_dict])\n",
    "    open('data/reports_cleaned/mda/' + file.split(os.sep)[3], 'w').write(f_clean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "# iterating through each file\n",
    "\n",
    "for i, row in cik_list.iterrows():\n",
    "    f = open('data/reports_cleaned/mda/' + row['SECFNAME'].split(os.sep)[3], 'r').read()\n",
    "    doc_dict = Counter(word_tokenize(f))\n",
    "    \n",
    "    # getting positive score\n",
    "    positive_f_dict = doc_dict & positive_dict\n",
    "    cik_list.loc[i, 'mda_positive_score'] = sum(positive_f_dict.values())\n",
    "    \n",
    "    # getting negative score\n",
    "    negative_f_dict = doc_dict & negative_dict\n",
    "    cik_list.loc[i, 'mda_negative_score'] = sum(negative_f_dict.values())\n",
    "    \n",
    "    # total words after cleaning\n",
    "    cik_list.loc[i, 'mda_total_words_after_cleaning'] = sum(doc_dict.values())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "cik_list['mda_polarity_score'] = cik_list\\\n",
    ".apply(lambda x: (x['mda_positive_score'] - x['mda_negative_score'])/((x['mda_positive_score'] \\\n",
    "                                                                       + x['mda_negative_score'])+0.000001),\n",
    "      axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "cik_list['mda_subjectivity_score'] = cik_list\\\n",
    ".apply(lambda x: (x['mda_positive_score'] + x['mda_negative_score'])/(x['mda_total_words_after_cleaning'] + 0.000001), \n",
    "       axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "cik_list['mda_sentiment_score_categorization'] = cik_list.mda_polarity_score.apply(sentiment_score_categorization)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "# getting number of sentences\n",
    "for i, row in cik_list.iterrows():\n",
    "    f = open('data/reports_cleaned/mda/' + row['SECFNAME'].split(os.sep)[3], 'r').read()\n",
    "    doc_dict = Counter(sent_tokenize(f))\n",
    "\n",
    "    # total words after cleaning\n",
    "    cik_list.loc[i, 'mda_total_sentences_after_cleaning'] = sum(doc_dict.values())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_avg_sent_length(x):\n",
    "    try:\n",
    "        return x['mda_total_words_after_cleaning']/x['mda_total_sentences_after_cleaning']\n",
    "    except:\n",
    "        return 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "cik_list['mda_average_sentence_length'] = cik_list\\\n",
    ".apply(lambda x: get_avg_sent_length(x), axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, row in cik_list.iterrows():\n",
    "    f = open('data/reports_cleaned/mda/' + row['SECFNAME'].split(os.sep)[3], 'r').read()\n",
    "    doc_dict = Counter(word_tokenize(f))\n",
    "    \n",
    "    complex_words = (1 for word in doc_dict if len(dic.inserted(word)) > 2)\n",
    "    cik_list.loc[i, 'mda_complex_word_count'] = sum(complex_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "cik_list['mda_percentage_of_complex_words'] = cik_list['mda_complex_word_count']/cik_list['mda_total_words_after_cleaning']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "cik_list['mda_fog_index'] = 0.4 * (cik_list['mda_average_sentence_length'] + cik_list['mda_percentage_of_complex_words'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, row in cik_list.iterrows():\n",
    "    f = open('data/reports_cleaned/mda/' + row['SECFNAME'].split(os.sep)[3], 'r').read()\n",
    "    doc_dict = Counter(word_tokenize(f))\n",
    "    \n",
    "    # removing stop words and counting words\n",
    "    word_count = (1 for word in doc_dict if word not in stopwords_dict_nltk and word not in punctuation_dict)\n",
    "    cik_list.loc[i, 'mda_word_count'] = sum(word_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, row in cik_list.iterrows():\n",
    "    f = open('data/reports_cleaned/mda/' + row['SECFNAME'].split(os.sep)[3], 'r').read()\n",
    "    doc_dict = Counter(word_tokenize(f))\n",
    "    \n",
    "    # getting uncertainity\n",
    "    uncertainty_f_dict = doc_dict & uncertainty_dict\n",
    "    cik_list.loc[i, 'mda_uncertainty_score'] = sum(uncertainty_f_dict.values())\n",
    "    \n",
    "    # getting constraining\n",
    "    constraining_f_dict = doc_dict & constraining_dict\n",
    "    cik_list.loc[i, 'mda_constraining_score'] = sum(constraining_f_dict.values())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### QQDMR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "# iterating through each file, removing stop words and saving it\n",
    "\n",
    "for file in cik_list.SECFNAME:\n",
    "    f = open('data/reports/qqdmr/' + file.split(os.sep)[3], 'r').read()\n",
    "    f_clean = ' '.join([word for word in f.split() if word not in stopwords_dict])\n",
    "    open('data/reports_cleaned/qqdmr/' + file.split(os.sep)[3], 'w').write(f_clean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "# iterating through each file\n",
    "\n",
    "for i, row in cik_list.iterrows():\n",
    "    f = open('data/reports_cleaned/qqdmr/' + row['SECFNAME'].split(os.sep)[3], 'r').read()\n",
    "    doc_dict = Counter(word_tokenize(f))\n",
    "    \n",
    "    # getting positive score\n",
    "    positive_f_dict = doc_dict & positive_dict\n",
    "    cik_list.loc[i, 'qqdmr_positive_score'] = sum(positive_f_dict.values())\n",
    "    \n",
    "    # getting negative score\n",
    "    negative_f_dict = doc_dict & negative_dict\n",
    "    cik_list.loc[i, 'qqdmr_negative_score'] = sum(negative_f_dict.values())\n",
    "    \n",
    "    # total words after cleaning\n",
    "    cik_list.loc[i, 'qqdmr_total_words_after_cleaning'] = sum(doc_dict.values())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "cik_list['qqdmr_polarity_score'] = cik_list\\\n",
    ".apply(lambda x: (x['qqdmr_positive_score'] - x['qqdmr_negative_score'])/((x['qqdmr_positive_score'] \\\n",
    "                                                                       + x['qqdmr_negative_score'])+0.000001),\n",
    "      axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "cik_list['qqdmr_subjectivity_score'] = cik_list\\\n",
    ".apply(lambda x: (x['qqdmr_positive_score'] + x['qqdmr_negative_score'])/(x['qqdmr_total_words_after_cleaning'] + 0.000001), \n",
    "       axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "cik_list['qqdmr_sentiment_score_categorization'] = cik_list.qqdmr_polarity_score.apply(sentiment_score_categorization)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "# getting number of sentences\n",
    "for i, row in cik_list.iterrows():\n",
    "    f = open('data/reports_cleaned/qqdmr/' + row['SECFNAME'].split(os.sep)[3], 'r').read()\n",
    "    doc_dict = Counter(sent_tokenize(f))\n",
    "\n",
    "    # total words after cleaning\n",
    "    cik_list.loc[i, 'qqdmr_total_sentences_after_cleaning'] = sum(doc_dict.values())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_avg_sent_length(x):\n",
    "    try:\n",
    "        return x['qqdmr_total_words_after_cleaning']/x['qqdmr_total_sentences_after_cleaning']\n",
    "    except:\n",
    "        return 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "cik_list['qqdmr_average_sentence_length'] = cik_list\\\n",
    ".apply(lambda x: get_avg_sent_length(x), axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, row in cik_list.iterrows():\n",
    "    f = open('data/reports_cleaned/qqdmr/' + row['SECFNAME'].split(os.sep)[3], 'r').read()\n",
    "    doc_dict = Counter(word_tokenize(f))\n",
    "    \n",
    "    complex_words = (1 for word in doc_dict if len(dic.inserted(word)) > 2)\n",
    "    cik_list.loc[i, 'qqdmr_complex_word_count'] = sum(complex_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "cik_list['qqdmr_percentage_of_complex_words'] = cik_list['qqdmr_complex_word_count']/cik_list['qqdmr_total_words_after_cleaning']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "cik_list['qqdmr_fog_index'] = 0.4 * (cik_list['qqdmr_average_sentence_length'] + cik_list['qqdmr_percentage_of_complex_words'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, row in cik_list.iterrows():\n",
    "    f = open('data/reports_cleaned/qqdmr/' + row['SECFNAME'].split(os.sep)[3], 'r').read()\n",
    "    doc_dict = Counter(word_tokenize(f))\n",
    "    \n",
    "    # removing stop words and counting words\n",
    "    word_count = (1 for word in doc_dict if word not in stopwords_dict_nltk and word not in punctuation_dict)\n",
    "    cik_list.loc[i, 'qqdmr_word_count'] = sum(word_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, row in cik_list.iterrows():\n",
    "    f = open('data/reports_cleaned/qqdmr/' + row['SECFNAME'].split(os.sep)[3], 'r').read()\n",
    "    doc_dict = Counter(word_tokenize(f))\n",
    "    \n",
    "    # getting uncertainity\n",
    "    uncertainty_f_dict = doc_dict & uncertainty_dict\n",
    "    cik_list.loc[i, 'qqdmr_uncertainty_score'] = sum(uncertainty_f_dict.values())\n",
    "    \n",
    "    # getting constraining\n",
    "    constraining_f_dict = doc_dict & constraining_dict\n",
    "    cik_list.loc[i, 'qqdmr_constraining_score'] = sum(constraining_f_dict.values())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "# iterating through each file, removing stop words and saving it\n",
    "\n",
    "for file in cik_list.SECFNAME:\n",
    "    f = open('data/reports/rf/' + file.split(os.sep)[3], 'r').read()\n",
    "    f_clean = ' '.join([word for word in f.split() if word not in stopwords_dict])\n",
    "    open('data/reports_cleaned/rf/' + file.split(os.sep)[3], 'w').write(f_clean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "# iterating through each file\n",
    "\n",
    "for i, row in cik_list.iterrows():\n",
    "    f = open('data/reports_cleaned/rf/' + row['SECFNAME'].split(os.sep)[3], 'r').read()\n",
    "    doc_dict = Counter(word_tokenize(f))\n",
    "    \n",
    "    # getting positive score\n",
    "    positive_f_dict = doc_dict & positive_dict\n",
    "    cik_list.loc[i, 'rf_positive_score'] = sum(positive_f_dict.values())\n",
    "    \n",
    "    # getting negative score\n",
    "    negative_f_dict = doc_dict & negative_dict\n",
    "    cik_list.loc[i, 'rf_negative_score'] = sum(negative_f_dict.values())\n",
    "    \n",
    "    # total words after cleaning\n",
    "    cik_list.loc[i, 'rf_total_words_after_cleaning'] = sum(doc_dict.values())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "cik_list['rf_polarity_score'] = cik_list\\\n",
    ".apply(lambda x: (x['rf_positive_score'] - x['rf_negative_score'])/((x['rf_positive_score'] \\\n",
    "                                                                       + x['rf_negative_score'])+0.000001),\n",
    "      axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "cik_list['rf_subjectivity_score'] = cik_list\\\n",
    ".apply(lambda x: (x['rf_positive_score'] + x['rf_negative_score'])/(x['rf_total_words_after_cleaning'] + 0.000001), \n",
    "       axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "cik_list['rf_sentiment_score_categorization'] = cik_list.rf_polarity_score.apply(sentiment_score_categorization)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "# getting number of sentences\n",
    "for i, row in cik_list.iterrows():\n",
    "    f = open('data/reports_cleaned/rf/' + row['SECFNAME'].split(os.sep)[3], 'r').read()\n",
    "    doc_dict = Counter(sent_tokenize(f))\n",
    "\n",
    "    # total words after cleaning\n",
    "    cik_list.loc[i, 'rf_total_sentences_after_cleaning'] = sum(doc_dict.values())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_avg_sent_length(x):\n",
    "    try:\n",
    "        return x['rf_total_words_after_cleaning']/x['rf_total_sentences_after_cleaning']\n",
    "    except:\n",
    "        return 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "cik_list['rf_average_sentence_length'] = cik_list\\\n",
    ".apply(lambda x: get_avg_sent_length(x), axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, row in cik_list.iterrows():\n",
    "    f = open('data/reports_cleaned/rf/' + row['SECFNAME'].split(os.sep)[3], 'r').read()\n",
    "    doc_dict = Counter(word_tokenize(f))\n",
    "    \n",
    "    complex_words = (1 for word in doc_dict if len(dic.inserted(word)) > 2)\n",
    "    cik_list.loc[i, 'rf_complex_word_count'] = sum(complex_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "cik_list['rf_percentage_of_complex_words'] = cik_list['rf_complex_word_count']/cik_list['rf_total_words_after_cleaning']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "cik_list['rf_fog_index'] = 0.4 * (cik_list['rf_average_sentence_length'] + cik_list['rf_percentage_of_complex_words'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, row in cik_list.iterrows():\n",
    "    f = open('data/reports_cleaned/rf/' + row['SECFNAME'].split(os.sep)[3], 'r').read()\n",
    "    doc_dict = Counter(word_tokenize(f))\n",
    "    \n",
    "    # removing stop words and counting words\n",
    "    word_count = (1 for word in doc_dict if word not in stopwords_dict_nltk and word not in punctuation_dict)\n",
    "    cik_list.loc[i, 'rf_word_count'] = sum(word_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, row in cik_list.iterrows():\n",
    "    f = open('data/reports_cleaned/rf/' + row['SECFNAME'].split(os.sep)[3], 'r').read()\n",
    "    doc_dict = Counter(word_tokenize(f))\n",
    "    \n",
    "    # getting uncertainity\n",
    "    uncertainty_f_dict = doc_dict & uncertainty_dict\n",
    "    cik_list.loc[i, 'rf_uncertainty_score'] = sum(uncertainty_f_dict.values())\n",
    "    \n",
    "    # getting constraining\n",
    "    constraining_f_dict = doc_dict & constraining_dict\n",
    "    cik_list.loc[i, 'rf_constraining_score'] = sum(constraining_f_dict.values())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Additional Variables: positive/negative and uncertainty/constraining word proportion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "# word proportions\n",
    "\n",
    "cik_list['mda_positive_word_proportion'] = cik_list['mda_positive_score']/cik_list['mda_word_count']\n",
    "cik_list['mda_negative_word_proportion'] = cik_list['mda_negative_score']/cik_list['mda_word_count']\n",
    "cik_list['qqdmr_positive_word_proportion'] = cik_list['qqdmr_positive_score']/cik_list['qqdmr_word_count']\n",
    "cik_list['qqdmr_negative_word_proportion'] = cik_list['qqdmr_negative_score']/cik_list['qqdmr_word_count']\n",
    "cik_list['rf_positive_word_proportion'] = cik_list['rf_positive_score']/cik_list['rf_word_count']\n",
    "cik_list['rf_negative_word_proportion'] = cik_list['rf_negative_score']/cik_list['rf_word_count']\n",
    "\n",
    "cik_list['mda_uncertainty_word_proportion'] = cik_list['mda_uncertainty_score']/cik_list['mda_word_count']\n",
    "cik_list['mda_constraining_word_proportion'] = cik_list['mda_constraining_score']/cik_list['mda_word_count']\n",
    "cik_list['qqdmr_uncertainty_word_proportion'] = cik_list['qqdmr_uncertainty_score']/cik_list['qqdmr_word_count']\n",
    "cik_list['qqdmr_constraining_word_proportion'] = cik_list['qqdmr_constraining_score']/cik_list['qqdmr_word_count']\n",
    "cik_list['rf_uncertainty_word_proportion'] = cik_list['rf_uncertainty_score']/cik_list['rf_word_count']\n",
    "cik_list['rf_constraining_word_proportion'] = cik_list['rf_constraining_score']/cik_list['rf_word_count']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Additional Variable: Constraining words for whole report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "# constraining_words_whole_report\n",
    "\n",
    "for i, row in cik_list.iterrows():\n",
    "    f = open('data/reports_cleaned/' + row['SECFNAME'].split(os.sep)[3], 'r').read()\n",
    "    doc_dict = Counter(word_tokenize(f))\n",
    "\n",
    "    # getting constraining\n",
    "    constraining_f_dict = doc_dict & constraining_dict\n",
    "    cik_list.loc[i, 'constraining_words_whole_report'] = sum(constraining_f_dict.values())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Output Data Structure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "# filling blanks with NAs\n",
    "\n",
    "cik_list = cik_list.fillna(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "original_columns = list(pd.read_excel('data/cik_list.xlsx').columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_columns = [\n",
    "    'mda_positive_score',\n",
    "    'mda_negative_score',\n",
    "    'mda_polarity_score',\n",
    "    'mda_average_sentence_length',\n",
    "    'mda_percentage_of_complex_words',\n",
    "    'mda_fog_index',\n",
    "    'mda_complex_word_count',\n",
    "    'mda_word_count',\n",
    "    'mda_uncertainty_score',\n",
    "    'mda_constraining_score',\n",
    "    'mda_positive_word_proportion',\n",
    "    'mda_negative_word_proportion',\n",
    "    'mda_uncertainty_word_proportion',\n",
    "    'mda_constraining_word_proportion',\n",
    "    'qqdmr_positive_score',\n",
    "    'qqdmr_negative_score',\n",
    "    'qqdmr_polarity_score',\n",
    "    'qqdmr_average_sentence_length',\n",
    "    'qqdmr_percentage_of_complex_words',\n",
    "    'qqdmr_fog_index',\n",
    "    'qqdmr_complex_word_count',\n",
    "    'qqdmr_word_count',\n",
    "    'qqdmr_uncertainty_score',\n",
    "    'qqdmr_constraining_score',\n",
    "    'qqdmr_positive_word_proportion',\n",
    "    'qqdmr_negative_word_proportion',\n",
    "    'qqdmr_uncertainty_word_proportion',\n",
    "    'qqdmr_constraining_word_proportion',\n",
    "    'rf_positive_score',\n",
    "    'rf_negative_score',\n",
    "    'rf_polarity_score',\n",
    "    'rf_average_sentence_length',\n",
    "    'rf_percentage_of_complex_words',\n",
    "    'rf_fog_index',\n",
    "    'rf_complex_word_count',\n",
    "    'rf_word_count',\n",
    "    'rf_uncertainty_score',\n",
    "    'rf_constraining_score',\n",
    "    'rf_positive_word_proportion',\n",
    "    'rf_negative_word_proportion',\n",
    "    'rf_uncertainty_word_proportion',\n",
    "    'rf_constraining_word_proportion',\n",
    "    'constraining_words_whole_report'\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "keep_columns = original_columns + output_columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['CIK', 'CONAME', 'FYRMO', 'FDATE', 'FORM', 'SECFNAME',\n",
       "       'mda_positive_score', 'mda_negative_score', 'mda_polarity_score',\n",
       "       'mda_average_sentence_length', 'mda_complex_word_count',\n",
       "       'mda_percentage_of_complex_words', 'mda_fog_index', 'mda_word_count',\n",
       "       'mda_uncertainty_score', 'mda_constraining_score',\n",
       "       'qqdmr_positive_score', 'qqdmr_negative_score', 'qqdmr_polarity_score',\n",
       "       'qqdmr_average_sentence_length', 'qqdmr_complex_word_count',\n",
       "       'qqdmr_percentage_of_complex_words', 'qqdmr_fog_index',\n",
       "       'qqdmr_word_count', 'qqdmr_uncertainty_score',\n",
       "       'qqdmr_constraining_score', 'rf_positive_score', 'rf_negative_score',\n",
       "       'rf_polarity_score', 'rf_average_sentence_length',\n",
       "       'rf_complex_word_count', 'rf_percentage_of_complex_words',\n",
       "       'rf_fog_index', 'rf_word_count', 'rf_uncertainty_score',\n",
       "       'rf_constraining_score', 'mda_positive_word_proportion',\n",
       "       'mda_negative_word_proportion', 'qqdmr_positive_word_proportion',\n",
       "       'qqdmr_negative_word_proportion', 'rf_positive_word_proportion',\n",
       "       'rf_negative_word_proportion', 'mda_uncertainty_word_proportion',\n",
       "       'mda_constraining_word_proportion', 'qqdmr_uncertainty_word_proportion',\n",
       "       'qqdmr_constraining_word_proportion', 'rf_uncertainty_word_proportion',\n",
       "       'rf_constraining_word_proportion', 'constraining_words_whole_report'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cik_list.columns.intersection(keep_columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "cik_list = cik_list[cik_list.columns.intersection(keep_columns)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "cik_list.to_csv('out_data_structure.csv', index = False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
